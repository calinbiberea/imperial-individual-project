{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6b41ca-5f07-4710-baae-24a1ed8dda14",
   "metadata": {},
   "source": [
    "# MNIST: Training and Testing on a Clean Dataset & Adversarial Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b34ed-646a-4a6d-9804-456031bbbe7f",
   "metadata": {},
   "source": [
    "## Imports and MNIST loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc6f794-8ded-4bca-bd88-62c136ee6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports all the module paths\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Loads the rest of the modules\n",
    "\n",
    "# File containing all the required training methods\n",
    "import defences.mnist as defences\n",
    "\n",
    "# For testing\n",
    "import utils.clean_test as clean_test\n",
    "\n",
    "# Contains the data loadders\n",
    "import utils.dataloaders as dataloaders\n",
    "\n",
    "# For printing outcomes\n",
    "# import utils.printing as printing\n",
    "\n",
    "# Example printing, but I removed it to simplify results\n",
    "# for epsilon in epsilons:\n",
    "#     printing.print_attack(\n",
    "#         model,\n",
    "#         testSetLoader,\n",
    "#         \"FGSM\",\n",
    "#         attacks[\"FGSM\"],\n",
    "#         epsilon=epsilon,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b26f2046-3c5c-435a-a11d-bbeb41de6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook will use PyTorch Device: CUDA\n"
     ]
    }
   ],
   "source": [
    "# Define the `device` PyTorch will be running on, please hope it is CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Notebook will use PyTorch Device: \" + device.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fee513-bdec-474f-9a4b-9c7a7321b7f2",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3cf35b0-a90d-4e04-b2d8-57e52f553b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../../datasets/\"\n",
    "\n",
    "trainSetLoader, _, testSetLoader = dataloaders.get_MNIST_data_loaders(\n",
    "    DATA_ROOT,\n",
    "    trainSetSize=50000,\n",
    "    validationSetSize=0,\n",
    "    batchSize=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69771c3a-ca1e-43c9-8808-ceceab79e38f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3d9a8d-5792-4d0d-8eec-43bb657b0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found already trained model...\n",
      "... loaded!\n"
     ]
    }
   ],
   "source": [
    "SAVE_LOAD_ROOT = \"../../models_data/MNIST\"\n",
    "\n",
    "model = defences.standard_training(\n",
    "    trainSetLoader,\n",
    "    load_if_available=True,\n",
    "    load_path=SAVE_LOAD_ROOT + \"/mnist_standard_with_feature_list\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f6411d-741f-4823-a737-bf74667695a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... done! Accuracy: 99.23%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "clean_test.test_trained_model(model, testSetLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d14986-d539-4337-ab84-ea7b88747d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, SAVE_LOAD_ROOT + \"/mnist_standard_with_feature_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcacac-3dd2-41a9-bc83-fce1463d0e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attacks and Their Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48235d0-c883-4e64-bb29-9deef17c259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A possible attacks array (for nice printing):\n",
    "# Some attacks use a helper library\n",
    "import torchattacks\n",
    "\n",
    "import attacks.fgsm as fgsm\n",
    "import attacks.ifgsm as ifgsm\n",
    "import attacks.pgd as pgd\n",
    "import utils.attacking as attacking\n",
    "\n",
    "attacks = {}\n",
    "\n",
    "attacks[\"FGSM\"] = fgsm.fgsm_attack\n",
    "attacks[\"I-FGSM\"] = ifgsm.ifgsm_attack\n",
    "attacks[\"PGD\"] = pgd.pgd_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b093c27-01e3-4dac-9cab-f66bbd3c1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce some FGSM data that we then use to extract features\n",
    "random_noise_size = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512f8ff-3e0f-4e38-952c-aa3ca1164f98",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700f845-32e3-46f3-ae78-e83c0dbba39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mahalanobis_score(\n",
    "    model,\n",
    "    trainSetLoader,\n",
    "    num_classes,\n",
    "    outf,\n",
    "    net_type,\n",
    "    sample_mean,\n",
    "    precision,\n",
    "    layer_index,\n",
    "    magnitude,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the proposed Mahalanobis confidence score on adversarial samples\n",
    "    return: Mahalanobis score from layer_index\n",
    "    \"\"\"\n",
    "\n",
    "    Mahalanobis = []\n",
    "    batch_size = 128\n",
    "    total = 0\n",
    "    # Go through the data to produce the scores\n",
    "    for _, (images, labels) in enumerate(tqdm(trainSetLoader, desc=\"Batches\")):\n",
    "        # Cast to proper tensors\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images, labels = torch.autograd.Variable(images, requires_grad=True), Variable(labels)\n",
    "\n",
    "    for data_index in range(int(np.floor(test_data.size(0) / batch_size))):\n",
    "        target = test_label[total : total + batch_size].cuda()\n",
    "        data = test_data[total : total + batch_size].cuda()\n",
    "        total += batch_size\n",
    "        data, target = torch.autograd.Variable(data, requires_grad=True), Variable(target)\n",
    "        \n",
    "        out_features = model.intermediate_forward(data, layer_index)\n",
    "        out_features = out_features.view(out_features.size(0), out_features.size(1), -1)\n",
    "        out_features = torch.mean(out_features, 2)\n",
    "\n",
    "        gaussian_score = 0\n",
    "        for i in range(num_classes):\n",
    "            batch_sample_mean = sample_mean[layer_index][i]\n",
    "            zero_f = out_features.data - batch_sample_mean\n",
    "            term_gau = (\n",
    "                -0.5\n",
    "                * torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "            )\n",
    "            if i == 0:\n",
    "                gaussian_score = term_gau.view(-1, 1)\n",
    "            else:\n",
    "                gaussian_score = torch.cat((gaussian_score, term_gau.view(-1, 1)), 1)\n",
    "\n",
    "        # Input_processing\n",
    "        sample_pred = gaussian_score.max(1)[1]\n",
    "        batch_sample_mean = sample_mean[layer_index].index_select(0, sample_pred)\n",
    "        zero_f = out_features - Variable(batch_sample_mean)\n",
    "        pure_gau = (\n",
    "            -0.5\n",
    "            * torch.mm(\n",
    "                torch.mm(zero_f, Variable(precision[layer_index])), zero_f.t()\n",
    "            ).diag()\n",
    "        )\n",
    "        loss = torch.mean(-pure_gau)\n",
    "        loss.backward()\n",
    "\n",
    "        gradient = torch.ge(data.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        if net_type == \"densenet\":\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([0]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([0]).cuda()) / (63.0 / 255.0),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([1]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([1]).cuda()) / (62.1 / 255.0),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([2]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([2]).cuda()) / (66.7 / 255.0),\n",
    "            )\n",
    "        elif net_type == \"resnet\":\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([0]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([0]).cuda()) / (0.2023),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([1]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([1]).cuda()) / (0.1994),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([2]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([2]).cuda()) / (0.2010),\n",
    "            )\n",
    "        tempInputs = torch.add(data.data, -magnitude, gradient)\n",
    "\n",
    "        noise_out_features = model.intermediate_forward(\n",
    "            Variable(tempInputs, volatile=True), layer_index\n",
    "        )\n",
    "        noise_out_features = noise_out_features.view(\n",
    "            noise_out_features.size(0), noise_out_features.size(1), -1\n",
    "        )\n",
    "        noise_out_features = torch.mean(noise_out_features, 2)\n",
    "        noise_gaussian_score = 0\n",
    "        for i in range(num_classes):\n",
    "            batch_sample_mean = sample_mean[layer_index][i]\n",
    "            zero_f = noise_out_features.data - batch_sample_mean\n",
    "            term_gau = (\n",
    "                -0.5\n",
    "                * torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "            )\n",
    "            if i == 0:\n",
    "                noise_gaussian_score = term_gau.view(-1, 1)\n",
    "            else:\n",
    "                noise_gaussian_score = torch.cat(\n",
    "                    (noise_gaussian_score, term_gau.view(-1, 1)), 1\n",
    "                )\n",
    "\n",
    "        noise_gaussian_score, _ = torch.max(noise_gaussian_score, dim=1)\n",
    "        Mahalanobis.extend(noise_gaussian_score.cpu().numpy())\n",
    "\n",
    "    return Mahalanobis\n",
    "\n",
    "        out_features = model.intermediate_forward(data, layer_index)\n",
    "        out_features = out_features.view(out_features.size(0), out_features.size(1), -1)\n",
    "        out_features = torch.mean(out_features, 2)\n",
    "\n",
    "        gaussian_score = 0\n",
    "        for i in range(num_classes):\n",
    "            batch_sample_mean = sample_mean[layer_index][i]\n",
    "            zero_f = out_features.data - batch_sample_mean\n",
    "            term_gau = (\n",
    "                -0.5\n",
    "                * torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "            )\n",
    "            if i == 0:\n",
    "                gaussian_score = term_gau.view(-1, 1)\n",
    "            else:\n",
    "                gaussian_score = torch.cat((gaussian_score, term_gau.view(-1, 1)), 1)\n",
    "\n",
    "        # Input_processing\n",
    "        sample_pred = gaussian_score.max(1)[1]\n",
    "        batch_sample_mean = sample_mean[layer_index].index_select(0, sample_pred)\n",
    "        zero_f = out_features - Variable(batch_sample_mean)\n",
    "        pure_gau = (\n",
    "            -0.5\n",
    "            * torch.mm(\n",
    "                torch.mm(zero_f, Variable(precision[layer_index])), zero_f.t()\n",
    "            ).diag()\n",
    "        )\n",
    "        loss = torch.mean(-pure_gau)\n",
    "        loss.backward()\n",
    "\n",
    "        gradient = torch.ge(data.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        if net_type == \"densenet\":\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([0]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([0]).cuda()) / (63.0 / 255.0),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([1]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([1]).cuda()) / (62.1 / 255.0),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([2]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([2]).cuda()) / (66.7 / 255.0),\n",
    "            )\n",
    "        elif net_type == \"resnet\":\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([0]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([0]).cuda()) / (0.2023),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([1]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([1]).cuda()) / (0.1994),\n",
    "            )\n",
    "            gradient.index_copy_(\n",
    "                1,\n",
    "                torch.LongTensor([2]).cuda(),\n",
    "                gradient.index_select(1, torch.LongTensor([2]).cuda()) / (0.2010),\n",
    "            )\n",
    "        tempInputs = torch.add(data.data, -magnitude, gradient)\n",
    "\n",
    "        noise_out_features = model.intermediate_forward(\n",
    "            Variable(tempInputs, volatile=True), layer_index\n",
    "        )\n",
    "        noise_out_features = noise_out_features.view(\n",
    "            noise_out_features.size(0), noise_out_features.size(1), -1\n",
    "        )\n",
    "        noise_out_features = torch.mean(noise_out_features, 2)\n",
    "        noise_gaussian_score = 0\n",
    "        for i in range(num_classes):\n",
    "            batch_sample_mean = sample_mean[layer_index][i]\n",
    "            zero_f = noise_out_features.data - batch_sample_mean\n",
    "            term_gau = (\n",
    "                -0.5\n",
    "                * torch.mm(torch.mm(zero_f, precision[layer_index]), zero_f.t()).diag()\n",
    "            )\n",
    "            if i == 0:\n",
    "                noise_gaussian_score = term_gau.view(-1, 1)\n",
    "            else:\n",
    "                noise_gaussian_score = torch.cat(\n",
    "                    (noise_gaussian_score, term_gau.view(-1, 1)), 1\n",
    "                )\n",
    "\n",
    "        noise_gaussian_score, _ = torch.max(noise_gaussian_score, dim=1)\n",
    "        Mahalanobis.extend(noise_gaussian_score.cpu().numpy())\n",
    "\n",
    "    return Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b9db298-0c6f-4f2d-9eea-5ce053851f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Mahalanobis distance features\n",
    "\n",
    "# Step 1: set the extraction parameters\n",
    "fake_input = torch.rand(2, 1, 28, 28).to(device)\n",
    "fake_input = Variable(fake_input)\n",
    "\n",
    "# Step 2: produce the feature list\n",
    "_, layer_list = model.feature_list(fake_input)\n",
    "num_output = len(layer_list)\n",
    "feature_list = np.empty(num_output)\n",
    "\n",
    "# Count the number of features\n",
    "count = 0\n",
    "\n",
    "# Track number of features and size (i.e. neurons really)\n",
    "for layer in layer_list:\n",
    "    feature_list[count] = layer.size(1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bca35-aad9-486d-81e0-30b07cea2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several magnitudes to experiment with\n",
    "magnitude_list = [0.0, 0.01, 0.005, 0.002, 0.0014, 0.001, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e58839-1816-4661-afc6-39fd67fe85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for magnitude in magnitude_list:\n",
    "    print(\"\\nNoise: \" + str(magnitude))\n",
    "    for i in range(num_output):\n",
    "        M_in = lib_generation.get_Mahalanobis_score_adv(\n",
    "            model,\n",
    "            test_clean_data,\n",
    "            test_label,\n",
    "            args.num_classes,\n",
    "            args.outf,\n",
    "            args.net_type,\n",
    "            sample_mean,\n",
    "            precision,\n",
    "            i,\n",
    "            magnitude,\n",
    "        )\n",
    "        M_in = np.asarray(M_in, dtype=np.float32)\n",
    "        if i == 0:\n",
    "            Mahalanobis_in = M_in.reshape((M_in.shape[0], -1))\n",
    "        else:\n",
    "            Mahalanobis_in = np.concatenate(\n",
    "                (Mahalanobis_in, M_in.reshape((M_in.shape[0], -1))), axis=1\n",
    "            )\n",
    "\n",
    "    for i in range(num_output):\n",
    "        M_out = lib_generation.get_Mahalanobis_score_adv(\n",
    "            model,\n",
    "            test_adv_data,\n",
    "            test_label,\n",
    "            args.num_classes,\n",
    "            args.outf,\n",
    "            args.net_type,\n",
    "            sample_mean,\n",
    "            precision,\n",
    "            i,\n",
    "            magnitude,\n",
    "        )\n",
    "        M_out = np.asarray(M_out, dtype=np.float32)\n",
    "        if i == 0:\n",
    "            Mahalanobis_out = M_out.reshape((M_out.shape[0], -1))\n",
    "        else:\n",
    "            Mahalanobis_out = np.concatenate(\n",
    "                (Mahalanobis_out, M_out.reshape((M_out.shape[0], -1))), axis=1\n",
    "            )\n",
    "\n",
    "    for i in range(num_output):\n",
    "        M_noisy = lib_generation.get_Mahalanobis_score_adv(\n",
    "            model,\n",
    "            test_noisy_data,\n",
    "            test_label,\n",
    "            args.num_classes,\n",
    "            args.outf,\n",
    "            args.net_type,\n",
    "            sample_mean,\n",
    "            precision,\n",
    "            i,\n",
    "            magnitude,\n",
    "        )\n",
    "        M_noisy = np.asarray(M_noisy, dtype=np.float32)\n",
    "        if i == 0:\n",
    "            Mahalanobis_noisy = M_noisy.reshape((M_noisy.shape[0], -1))\n",
    "        else:\n",
    "            Mahalanobis_noisy = np.concatenate(\n",
    "                (Mahalanobis_noisy, M_noisy.reshape((M_noisy.shape[0], -1))), axis=1\n",
    "            )\n",
    "    Mahalanobis_in = np.asarray(Mahalanobis_in, dtype=np.float32)\n",
    "    Mahalanobis_out = np.asarray(Mahalanobis_out, dtype=np.float32)\n",
    "    Mahalanobis_noisy = np.asarray(Mahalanobis_noisy, dtype=np.float32)\n",
    "    Mahalanobis_pos = np.concatenate((Mahalanobis_in, Mahalanobis_noisy))\n",
    "\n",
    "    Mahalanobis_data, Mahalanobis_labels = lib_generation.merge_and_generate_labels(\n",
    "        Mahalanobis_out, Mahalanobis_pos\n",
    "    )\n",
    "    file_name = os.path.join(\n",
    "        args.outf,\n",
    "        \"Mahalanobis_%s_%s_%s.npy\" % (str(magnitude), args.dataset, args.adv_type),\n",
    "    )\n",
    "\n",
    "    Mahalanobis_data = np.concatenate((Mahalanobis_data, Mahalanobis_labels), axis=1)\n",
    "    np.save(file_name, Mahalanobis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
